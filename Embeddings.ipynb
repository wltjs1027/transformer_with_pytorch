{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"입력 토큰 인덱스를 임베딩 벡터로 변환하는 클래스.\n",
    "\n",
    "    주어진 단어 집합 크기(vocab_size)과 임베딩 차원(d_model)을 바탕으로 임베딩 레이어를 생성.\n",
    "    입력 토큰을 처리할 때 사용.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class Embeddings(nn.Module):\n",
    "    '''입력 토큰과 출력 토큰을 d_model 차원의 벡터로 변환하는 임베딩 레이어.\n",
    "\n",
    "    Attributes:\n",
    "        embedding(nn.Embedding): 임베딩 레이어로, 단어 집합 크기(vocab_size)와 임베딩 차원(d_model)을 인자로 받음.\n",
    "        d_model(int): 임베딩 벡터의 차원\n",
    "    '''\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \"\"\"Embeddings 클래스의 생성자\n",
    "        \n",
    "        Args:\n",
    "            vocab_size(int): 단어 집합 크기, 즉, vocab 사이즈\n",
    "            d_model(int): 임베딩 벡터의 차원\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = d_model) \n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"입력 토큰 인덱스를 임베딩 벡터로 변환하고 스케일링.\n",
    "\n",
    "        Args:\n",
    "            x(torch.Tensor): 입력 텐서로, 토큰의 정수형 인덱스 포함. (배치 크기, 시퀀스 길이)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: d_model 차원의 임베딩 벡터를 반환. (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "        \"\"\"\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) #attention mechanism의 안정성을 위해 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'example.', 'This', 'is', 'embedding'}\n",
      "{'example.': 1, 'This': 2, 'is': 3, 'embedding': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.9610, -0.8996, -0.8464],\n",
       "         [-0.2177,  1.2763, -0.1698],\n",
       "         [ 0.9773,  0.2740,  0.9070],\n",
       "         [-0.8541,  0.2068, -2.0466]], requires_grad=True),\n",
       " tensor([ 0.9610, -0.8996, -0.8464], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Embedding의 예시\n",
    "sentence = \"This is embedding example.\"\n",
    "word_set = set(sentence.split())\n",
    "print(word_set)\n",
    "\n",
    "vocab = {token: i+1 for i, token in enumerate(word_set)}\n",
    "print(vocab)\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=3)\n",
    "\n",
    "embedding_layer.weight, embedding_layer.weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[-0.4557,  0.0910,  0.3981, -0.0495, -1.2533, -0.3818],\n",
      "         [ 0.0997,  0.3162, -0.2746,  0.1792, -0.5500,  0.0124],\n",
      "         [-0.7752,  0.2090,  0.5267,  0.0403, -3.5858, -0.7578]],\n",
      "\n",
      "        [[ 2.0962, -1.0729,  0.2327,  0.7603,  2.0336, -0.9209],\n",
      "         [ 0.2261,  0.7553, -0.0878, -0.0246,  0.4555,  1.0830],\n",
      "         [-1.0600,  1.0272, -0.5820, -0.3202, -0.1183,  0.3251]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 10  \n",
    "d_model = 6 \n",
    "\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # (배치 크기 2, 시퀀스 길이 3)\n",
    "\n",
    "x_embedding = embedding(x)\n",
    "\n",
    "print(x_embedding.shape)  # 결과: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "print(x_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformer]",
   "language": "python",
   "name": "conda-env-transformer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
